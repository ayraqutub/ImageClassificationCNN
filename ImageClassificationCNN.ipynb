{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification with Convolutional Neural Networks (CNN) Using MNIST Dataset\n",
        "#### Ayra Qutub\n",
        "#### 1708104\n",
        "#### ECE 449 Lab D31\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Introduction**\n",
        "The objective of this lab was to build and train a Convolutional Neural Network (CNN) to perform image classification using the MNIST dataset, which contains 28x28 grayscale images of handwritten digits (0-9).\n",
        "\n",
        "CNNs are a powerful tool for image classification due to their ability to learn spatial hierarchies of features. The CNN architecture used in this report has a convolutional layer followed by a pooling layer, another set of the same, and then the final connection layer.\n",
        "\n",
        "The key tasks were to explore different hyperparameters, including the number of filters and learning rates, and to find the best-performing model using hyperperameter exploration. Once the optimal hyperparameters were identified, the model was trained on the entire dataset and evaluated on a test set.\n",
        "\n",
        "The CNN is formatted as an automated pipeline. It includes the expected steps of data ingestion, data preprocessing, model training, and model evaluation. Before training the final model, hyperperameter exploration is conducted to assess the combination of hyperperameters that result in the greatest accuracy. Each of these steps was modularized for better clarity and usability."
      ],
      "metadata": {
        "id": "8NW4DVNh7xur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "HgvhdKQARIo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Ingestion**\n",
        "\n",
        "We are working with the Modified National Institute of Standards and Technology (MNIST) database in this lab. This is a large database of handwritten numbers. It contains 60,000 training images and 10,000 testing images. The NIST has already split the data into the train and test sets. The function `data_ingestion` preserves this split and ingests the data to be used further in the pipeline."
      ],
      "metadata": {
        "id": "J3-MNjMd9uHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_ingestion():\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "metadata": {
        "id": "9gS-6M2MRWUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Preprocessing**\n",
        "Here, the data is processed using One-Hot Encoding. This converts the labels into vectors like `[0,0,1,0,...,0]` for multi-class classification. Additionally, the input is reshaped to a 4D array to be compatible with the CNN layers, where 1 represents the grayscale channel."
      ],
      "metadata": {
        "id": "xlgWifR7_kZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(X_train, y_train, X_test, y_test):\n",
        "\n",
        "  # convert class labels into one-hot encoding\n",
        "  y_train_encoded = to_categorical(y_train)\n",
        "  y_test_encoded = to_categorical(y_test)\n",
        "\n",
        "  # reshape the data to be in the form (samples, 28, 28, 1) for grayscale images\n",
        "  X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "  X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "\n",
        "  return X_train, y_train_encoded, X_test, y_test_encoded"
      ],
      "metadata": {
        "id": "mYDS_en6RxuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build Model**\n",
        "The model is a CNN with its architecture based on that of the one outlined in [1], with alternating convolution and pooling layers.\n",
        "\n",
        "The convolution layers use a ReLU activation function. This is the function `f(x)= { x | x > 0; 0 | otherwise }`. These layers extract features from the input images with filters. The filters are variables which will be inputted at the time of running. This makes it easy to automatically test different filters and decide on the best one to create our model.\n",
        "\n",
        "The pooling layers downsample the feature maps, reducing their dimensionality while retaining important features. These use a MAX() pooling function. This means that in the downsampling, the pooling preserves the greatest value within each region.\n",
        "\n",
        "The alternating convolution and pooling layers are followed by a fully connected layer. This maps the final feature maps to the output classes (digits 0-9) using a single dense layer and a softmax layer."
      ],
      "metadata": {
        "id": "0Icem3TiALuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(filters, learning_rate):\n",
        "    model = models.Sequential()\n",
        "    # Alternating convolution and pooling layers\n",
        "\n",
        "    # First convolution layer\n",
        "    model.add(layers.Conv2D(filters=filters, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    # First pooling layer\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolution layer\n",
        "    model.add(layers.Conv2D(filters=filters, kernel_size=(3, 3), activation='relu'))\n",
        "    # Second pooling layer\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Flattening and fully connected layer\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "kMMacFJZSKes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hyperperameter Exploration**\n",
        "The hyperperameters in this CNN are filters (also called kernels, designed to detect specific patterns or features in the input data) and learning rate (controls how much to change the model in response to the estimated error). These are the parameters responsible for directly influencing the model structure, functions, and performance. We can optimize the performance and accuracy of a model by choosing the correct hyperperameters. These hyperperameters are determined through hyperperameter exploration.\n",
        "\n",
        "To conduct this, the model is trained and tested for each combination of hyperperameters. The accuracy is recorded and the hyperperameter combination that results in the highest accuracy is saved. This will be used for the final model.\n",
        "\n",
        "The model is trained and validated using a Stratified K-Fold technique with 5 folds. The folds are creating by seperating the dataset into K (5) proportionate strata. These are combined into folds by taking the first stratum from each class and combining them into the first fold, the second stratum from each class into the second fold, and so on. This way, the folds reflect the dataset’s original class distribution. During validation, one fold serves as the test set while the other are used for training. This is iterated for each fold."
      ],
      "metadata": {
        "id": "GYJF1CpfCna4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperperameter_exploration(X_train, y_train_encoded, filter_options, learning_rate_options):\n",
        "  # Initialize Stratified KFold\n",
        "  skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "  # To track the best combination\n",
        "  results = {}\n",
        "  best_accuracy = 0\n",
        "  best_combination = None\n",
        "\n",
        "  for filters in filter_options:\n",
        "      for lr in learning_rate_options:\n",
        "          fold_no = 1\n",
        "          fold_accuracies = []\n",
        "          for train_index, val_index in skf.split(X_train, y_train):\n",
        "              print(f\"Training fold {fold_no} with {filters} filters and {lr} learning rate\")\n",
        "\n",
        "              X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "              y_train_fold, y_val_fold = y_train_encoded[train_index], y_train_encoded[val_index]\n",
        "\n",
        "              # Build and train the model with the current fold\n",
        "              model = build_model(filters, lr)\n",
        "              model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=5)\n",
        "\n",
        "              # Evaluate the model\n",
        "              val_loss, val_acc = model.evaluate(X_val_fold, y_val_fold)\n",
        "              print(f\"Fold {fold_no} - Validation accuracy: {val_acc}\")\n",
        "              fold_accuracies.append(val_acc)\n",
        "              fold_no += 1\n",
        "          avg_acc = np.mean(fold_accuracies)\n",
        "          print(f\"Average validation accuracy for {filters} filters and {lr} learning rate: {avg_acc}\")\n",
        "          print(\"-\" * 50)\n",
        "          # Store the result in a dictionary\n",
        "          results[(filters, lr)] = avg_acc\n",
        "\n",
        "          # Track the best combination\n",
        "          if avg_acc > best_accuracy:\n",
        "              best_accuracy = avg_acc\n",
        "              best_combination = (filters, lr)\n",
        "\n",
        "  # After all combinations are tested\n",
        "  return best_combination, best_accuracy"
      ],
      "metadata": {
        "id": "sQ0lp0YYUSP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Training**\n",
        "After completing hyperperameter exploration, the final model is trained with the hyperperameters that gave the highest accuracy; the model is trained on the entire dataset for 10 epochs."
      ],
      "metadata": {
        "id": "fErUhcYsFIhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_training(best_combination, X_train, y_train_encoded):\n",
        "  best_model = build_model(*best_combination)\n",
        "  best_model.fit(X_train, y_train_encoded, epochs=10)\n",
        "  return best_model"
      ],
      "metadata": {
        "id": "We5vUYt1bOQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Evaluation**\n",
        "The final model is evaluated on the test set to measure its generalization performance. This evaluates the model’s performance on unseen data, giving the final accuracy and loss values."
      ],
      "metadata": {
        "id": "mPc_M_1rFYJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(best_model, X_test, y_test_encoded):\n",
        "  loss, accuracy = best_model.evaluate(X_test, y_test_encoded)\n",
        "  return loss, accuracy"
      ],
      "metadata": {
        "id": "bhJF-bZnbaxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pipeline**\n",
        "As a final step, all of the above is automated and put into a pipeline function, which, in this case, we will be calling the main function. This does all the steps we set out to do: It takes in the data, processes it, conducts hyperperameter exploration, trains a model with the selected hyperperameters, and then evaluates the model."
      ],
      "metadata": {
        "id": "zLaOtNNpFtbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  X_train, y_train, X_test, y_test = data_ingestion()\n",
        "  X_train, y_train_encoded, X_test, y_test_encoded = data_preprocessing(X_train, y_train, X_test, y_test)\n",
        "  filter_options = [16, 32]\n",
        "  learning_rate_options = [0.001, 0.01]\n",
        "  best_combination, best_accuracy = hyperperameter_exploration(X_train, y_train_encoded, filter_options, learning_rate_options)\n",
        "  print(f\"\\nBest combination: {best_combination} with accuracy: {best_accuracy}\")\n",
        "  best_model = model_training(best_combination, X_train, y_train_encoded)\n",
        "  loss, accuracy = model_evaluation(best_model, X_test, y_test_encoded)\n",
        "  print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnlwRFXmbh_1",
        "outputId": "feb7b8f9-9287-49c6-bb21-01d8d74dc076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1 with 16 filters and 0.001 learning rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 24ms/step - accuracy: 0.7728 - loss: 2.8144 - val_accuracy: 0.9400 - val_loss: 0.2134\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 17ms/step - accuracy: 0.9638 - loss: 0.1275 - val_accuracy: 0.9671 - val_loss: 0.1192\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 17ms/step - accuracy: 0.9746 - loss: 0.0826 - val_accuracy: 0.9758 - val_loss: 0.0839\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 16ms/step - accuracy: 0.9804 - loss: 0.0603 - val_accuracy: 0.9740 - val_loss: 0.0965\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.9834 - loss: 0.0512 - val_accuracy: 0.9750 - val_loss: 0.0890\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9746 - loss: 0.0853\n",
            "Fold 1 - Validation accuracy: 0.9750000238418579\n",
            "Training fold 2 with 16 filters and 0.001 learning rate\n",
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.7723 - loss: 2.7985 - val_accuracy: 0.9556 - val_loss: 0.1503\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 18ms/step - accuracy: 0.9642 - loss: 0.1260 - val_accuracy: 0.9691 - val_loss: 0.1028\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 17ms/step - accuracy: 0.9758 - loss: 0.0824 - val_accuracy: 0.9710 - val_loss: 0.0966\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 16ms/step - accuracy: 0.9802 - loss: 0.0653 - val_accuracy: 0.9770 - val_loss: 0.0732\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 17ms/step - accuracy: 0.9836 - loss: 0.0555 - val_accuracy: 0.9768 - val_loss: 0.0787\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9750 - loss: 0.0797\n",
            "Fold 2 - Validation accuracy: 0.9767500162124634\n",
            "Training fold 3 with 16 filters and 0.001 learning rate\n",
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 16ms/step - accuracy: 0.7315 - loss: 3.7133 - val_accuracy: 0.9502 - val_loss: 0.1758\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 17ms/step - accuracy: 0.9590 - loss: 0.1385 - val_accuracy: 0.9643 - val_loss: 0.1222\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 16ms/step - accuracy: 0.9712 - loss: 0.0915 - val_accuracy: 0.9716 - val_loss: 0.1000\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 18ms/step - accuracy: 0.9767 - loss: 0.0753 - val_accuracy: 0.9701 - val_loss: 0.1049\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 16ms/step - accuracy: 0.9798 - loss: 0.0650 - val_accuracy: 0.9748 - val_loss: 0.0987\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9756 - loss: 0.1077\n",
            "Fold 3 - Validation accuracy: 0.9748333096504211\n",
            "Training fold 4 with 16 filters and 0.001 learning rate\n",
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 17ms/step - accuracy: 0.7515 - loss: 3.3528 - val_accuracy: 0.9487 - val_loss: 0.1963\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 17ms/step - accuracy: 0.9586 - loss: 0.1471 - val_accuracy: 0.9596 - val_loss: 0.1399\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 16ms/step - accuracy: 0.9717 - loss: 0.0925 - val_accuracy: 0.9697 - val_loss: 0.1050\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 19ms/step - accuracy: 0.9773 - loss: 0.0729 - val_accuracy: 0.9686 - val_loss: 0.1059\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 17ms/step - accuracy: 0.9821 - loss: 0.0608 - val_accuracy: 0.9728 - val_loss: 0.0963\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9722 - loss: 0.1053\n",
            "Fold 4 - Validation accuracy: 0.9727500081062317\n",
            "Training fold 5 with 16 filters and 0.001 learning rate\n",
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.7604 - loss: 3.8084 - val_accuracy: 0.9530 - val_loss: 0.1655\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.9599 - loss: 0.1382 - val_accuracy: 0.9656 - val_loss: 0.1210\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 18ms/step - accuracy: 0.9712 - loss: 0.0917 - val_accuracy: 0.9778 - val_loss: 0.0818\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 17ms/step - accuracy: 0.9782 - loss: 0.0728 - val_accuracy: 0.9795 - val_loss: 0.0744\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 18ms/step - accuracy: 0.9817 - loss: 0.0592 - val_accuracy: 0.9798 - val_loss: 0.0767\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9777 - loss: 0.0808\n",
            "Fold 5 - Validation accuracy: 0.9798333048820496\n",
            "Average validation accuracy for 16 filters and 0.001 learning rate: 0.9758333325386047\n",
            "--------------------------------------------------\n",
            "Training fold 1 with 16 filters and 0.01 learning rate\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 16ms/step - accuracy: 0.7163 - loss: 2.5858 - val_accuracy: 0.8814 - val_loss: 0.4277\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 17ms/step - accuracy: 0.9198 - loss: 0.2762 - val_accuracy: 0.9247 - val_loss: 0.2628\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 17ms/step - accuracy: 0.9258 - loss: 0.2542 - val_accuracy: 0.9405 - val_loss: 0.2041\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.9288 - loss: 0.2520 - val_accuracy: 0.9335 - val_loss: 0.2211\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 16ms/step - accuracy: 0.9279 - loss: 0.2511 - val_accuracy: 0.9279 - val_loss: 0.2428\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9333 - loss: 0.2336\n",
            "Fold 1 - Validation accuracy: 0.9279166460037231\n",
            "Training fold 2 with 16 filters and 0.01 learning rate\n",
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 18ms/step - accuracy: 0.8008 - loss: 1.6183 - val_accuracy: 0.9098 - val_loss: 0.3005\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 16ms/step - accuracy: 0.9289 - loss: 0.2407 - val_accuracy: 0.9594 - val_loss: 0.1290\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 17ms/step - accuracy: 0.9464 - loss: 0.1823 - val_accuracy: 0.9388 - val_loss: 0.2015\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 15ms/step - accuracy: 0.9470 - loss: 0.1883 - val_accuracy: 0.9538 - val_loss: 0.1426\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 17ms/step - accuracy: 0.9480 - loss: 0.1720 - val_accuracy: 0.9428 - val_loss: 0.1771\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9440 - loss: 0.1788\n",
            "Fold 2 - Validation accuracy: 0.9428333044052124\n",
            "Training fold 3 with 16 filters and 0.01 learning rate\n",
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 18ms/step - accuracy: 0.7905 - loss: 1.5333 - val_accuracy: 0.9154 - val_loss: 0.2789\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 16ms/step - accuracy: 0.9257 - loss: 0.2697 - val_accuracy: 0.9046 - val_loss: 0.4154\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.9282 - loss: 0.2605 - val_accuracy: 0.9233 - val_loss: 0.2672\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 16ms/step - accuracy: 0.9157 - loss: 0.3188 - val_accuracy: 0.9170 - val_loss: 0.2743\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.9278 - loss: 0.2617 - val_accuracy: 0.9218 - val_loss: 0.2676\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9210 - loss: 0.2622\n",
            "Fold 3 - Validation accuracy: 0.921833336353302\n",
            "Training fold 4 with 16 filters and 0.01 learning rate\n",
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 16ms/step - accuracy: 0.7808 - loss: 1.8758 - val_accuracy: 0.9108 - val_loss: 0.3146\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 16ms/step - accuracy: 0.9274 - loss: 0.2522 - val_accuracy: 0.9381 - val_loss: 0.2357\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 17ms/step - accuracy: 0.9356 - loss: 0.2377 - val_accuracy: 0.9425 - val_loss: 0.2545\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.9343 - loss: 0.2544 - val_accuracy: 0.9461 - val_loss: 0.2125\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 17ms/step - accuracy: 0.9170 - loss: 0.3035 - val_accuracy: 0.9128 - val_loss: 0.2841\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9131 - loss: 0.2901\n",
            "Fold 4 - Validation accuracy: 0.9128333330154419\n",
            "Training fold 5 with 16 filters and 0.01 learning rate\n",
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.8297 - loss: 1.4047 - val_accuracy: 0.9482 - val_loss: 0.1930\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.9430 - loss: 0.2042 - val_accuracy: 0.9545 - val_loss: 0.1475\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 17ms/step - accuracy: 0.9496 - loss: 0.1679 - val_accuracy: 0.9548 - val_loss: 0.1510\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.9529 - loss: 0.1570 - val_accuracy: 0.9529 - val_loss: 0.1623\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.9528 - loss: 0.1556 - val_accuracy: 0.9513 - val_loss: 0.1551\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9447 - loss: 0.1684\n",
            "Fold 5 - Validation accuracy: 0.9512500166893005\n",
            "Average validation accuracy for 16 filters and 0.01 learning rate: 0.931333327293396\n",
            "--------------------------------------------------\n",
            "Training fold 1 with 32 filters and 0.001 learning rate\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 27ms/step - accuracy: 0.8373 - loss: 1.5079 - val_accuracy: 0.9649 - val_loss: 0.1291\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 24ms/step - accuracy: 0.9743 - loss: 0.0906 - val_accuracy: 0.9770 - val_loss: 0.0813\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 25ms/step - accuracy: 0.9800 - loss: 0.0654 - val_accuracy: 0.9758 - val_loss: 0.0968\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 25ms/step - accuracy: 0.9842 - loss: 0.0523 - val_accuracy: 0.9797 - val_loss: 0.0738\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 25ms/step - accuracy: 0.9845 - loss: 0.0462 - val_accuracy: 0.9786 - val_loss: 0.0771\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9797 - loss: 0.0763\n",
            "Fold 1 - Validation accuracy: 0.9785833358764648\n",
            "Training fold 2 with 32 filters and 0.001 learning rate\n",
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 26ms/step - accuracy: 0.8350 - loss: 2.2850 - val_accuracy: 0.9620 - val_loss: 0.1432\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 24ms/step - accuracy: 0.9678 - loss: 0.1114 - val_accuracy: 0.9726 - val_loss: 0.0956\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 26ms/step - accuracy: 0.9778 - loss: 0.0714 - val_accuracy: 0.9787 - val_loss: 0.0724\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 25ms/step - accuracy: 0.9826 - loss: 0.0585 - val_accuracy: 0.9706 - val_loss: 0.1015\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 25ms/step - accuracy: 0.9841 - loss: 0.0501 - val_accuracy: 0.9821 - val_loss: 0.0618\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9813 - loss: 0.0641\n",
            "Fold 2 - Validation accuracy: 0.9820833206176758\n",
            "Training fold 3 with 32 filters and 0.001 learning rate\n",
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 25ms/step - accuracy: 0.8466 - loss: 1.5346 - val_accuracy: 0.9682 - val_loss: 0.1243\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 26ms/step - accuracy: 0.9717 - loss: 0.0980 - val_accuracy: 0.9739 - val_loss: 0.0930\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 25ms/step - accuracy: 0.9783 - loss: 0.0689 - val_accuracy: 0.9797 - val_loss: 0.0757\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 25ms/step - accuracy: 0.9830 - loss: 0.0563 - val_accuracy: 0.9765 - val_loss: 0.0856\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 28ms/step - accuracy: 0.9852 - loss: 0.0461 - val_accuracy: 0.9790 - val_loss: 0.0879\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9785 - loss: 0.0940\n",
            "Fold 3 - Validation accuracy: 0.9789999723434448\n",
            "Training fold 4 with 32 filters and 0.001 learning rate\n",
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 25ms/step - accuracy: 0.8413 - loss: 2.0457 - val_accuracy: 0.9610 - val_loss: 0.1280\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9738 - loss: 0.0897 - val_accuracy: 0.9702 - val_loss: 0.1129\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9814 - loss: 0.0603 - val_accuracy: 0.9730 - val_loss: 0.1061\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 25ms/step - accuracy: 0.9835 - loss: 0.0550 - val_accuracy: 0.9749 - val_loss: 0.0930\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 26ms/step - accuracy: 0.9853 - loss: 0.0454 - val_accuracy: 0.9771 - val_loss: 0.0995\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9766 - loss: 0.1122\n",
            "Fold 4 - Validation accuracy: 0.9770833253860474\n",
            "Training fold 5 with 32 filters and 0.001 learning rate\n",
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 25ms/step - accuracy: 0.8364 - loss: 1.7702 - val_accuracy: 0.9706 - val_loss: 0.1070\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 26ms/step - accuracy: 0.9707 - loss: 0.0961 - val_accuracy: 0.9740 - val_loss: 0.0859\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 25ms/step - accuracy: 0.9796 - loss: 0.0660 - val_accuracy: 0.9768 - val_loss: 0.0781\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 24ms/step - accuracy: 0.9844 - loss: 0.0529 - val_accuracy: 0.9799 - val_loss: 0.0753\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 26ms/step - accuracy: 0.9849 - loss: 0.0470 - val_accuracy: 0.9750 - val_loss: 0.0956\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9732 - loss: 0.0970\n",
            "Fold 5 - Validation accuracy: 0.9750000238418579\n",
            "Average validation accuracy for 32 filters and 0.001 learning rate: 0.9783499956130981\n",
            "--------------------------------------------------\n",
            "Training fold 1 with 32 filters and 0.01 learning rate\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 25ms/step - accuracy: 0.7983 - loss: 2.7288 - val_accuracy: 0.9324 - val_loss: 0.2306\n",
            "Epoch 2/5\n",
            "\u001b[1m  10/1500\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 20ms/step - accuracy: 0.9469 - loss: 0.2171"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Results**\n",
        "After performing stratified cross-validation, the best hyperparameter combination was determined to be **32** filters with a learning rate of **0.001**, which achieved the highest average validation accuracy across the folds.\n",
        "\n",
        "After final training, the model was evaluated on the test set. The accuracy of the model, rounded to the nearest thousandth, is **0.985**.\n"
      ],
      "metadata": {
        "id": "9TKW7k6z0WZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusion**\n",
        "In this lab, we successfully built a CNN for image classification on the MNIST dataset. By experimenting with different hyperparameters using stratified cross-validation, we identified the best-performing model, which was able to generalize well to the test data, achieving about 98.5% accuracy. This lab demonstrated the effectiveness of CNNs in image classification tasks and provided practical experience in hyperparameter tuning and model evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "The MNIST dataset is a well-known benchmark in the field of machine learning and deep learning, with many models achieving high accuracy on the task of digit classification. The results obtained from the CNN built here can be compared with those reported in recent papers and current models for image classification.\n",
        "\n",
        "One such model is the Ensemble Network, as demonstrated in [2]. This paper proposes a novel architecture called EnsNet, designed to enhance the performance of Convolutional Neural Networks (CNNs) in image classification tasks. EnsNet combines a base CNN with multiple Fully Connected SubNetworks (FCSNs). The key idea is to split the feature maps generated by the last convolutional layer of the base CNN into disjoint subsets, which are then assigned to the FCSNs. Each FCSN is trained independently, and the final prediction is determined through majority voting between the base CNN and the FCSNs. This approach uses ensemble learning, introducing diversity among the learners by training FCSNs on different subsets of the feature maps.\n",
        "\n",
        "The EnsNet CNN achieved a state-of-the-art error rate of 0.16% (99.84% accuracy) on the MNIST dataset using this complex ensemble method.\n",
        "The lab CNN achieved around 98.5% accuracy, which is lower, as expected from a simpler architecture without ensemble learning or extensive regularization.\n",
        "\n",
        "Another recent paper, [3], explores techniques to enhance the performance of basic CNNs. This is done through data augmentation, dropout, and early stopping. The paper demonstrates that even plain CNNs, when combined with effective regularization and optimization techniques, can compete with more advanced architectures like residual networks.\n",
        "\n",
        "The study applies these regularization methods to several datasets, including MNIST, and achieves state-of-the-art performance on it 99.83% accuracy. In contrast, the CNN implemented in the lab is simpler, with only 2 convolutional layers and no regularization, achieving 98.5% accuracy. While the lab CNN performs well for its simplicity, the enhanced model shows that introducing regularization and optimization techniques significantly boosts performance and generalization, especially on larger datasets.\n",
        "\n",
        "While current state-of-the-art techniques outperform our model, these approaches require more complex architectures and computational resources. Given the simplicity of our model and the absence of advanced techniques like data augmentation or ensemble learning, achieving 98.5% is a strong result that demonstrates the effectiveness of CNNs even with basic configurations. This result is particularly impressive considering that our model was developed through basic stratified cross-validation and hyperparameter tuning, without the use of any advanced optimization techniques."
      ],
      "metadata": {
        "id": "cS9EAtRa03Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Resources**\n",
        "[1] Yann LeCun, Léon Bottou, Yoshua Bengio and Patrick Haffner: Gradient Based Learning Applied to Document Recognition, Proceedings of IEEE, 86(11):2278–2324, 1998.\n",
        "\n",
        "[2] D. Hirata and N. Takahashi, \"Ensemble learning in CNN augmented with fully connected subnetworks,\" arXiv preprint arXiv:2003.08562, Mar. 2020. [Online]. Available: https://arxiv.org/abs/2003.08562\n",
        "\n",
        "[3] Y. S. Assiri, \"Stochastic Optimization of Plain Convolutional Neural Networks with Simple Methods,\" arXiv preprint arXiv:2009.08589, Mar. 2020. [Online]. Available: https://arxiv.org/abs/2001.08856."
      ],
      "metadata": {
        "id": "I2keq5pmAXU5"
      }
    }
  ]
}